{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: DeepSeek R1 has up to 671 billion parameters in its flagship releases. [1]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import google.generativeai as genai\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "genai.configure(api_key=\"Enter Your Gemini API Key\")\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "SERPAPI_KEY = \"Enter Your SERP API Key\"\n",
    "\n",
    "class SerpApiRAG:\n",
    "    def __init__(self):\n",
    "        self.search_endpoint = \"https://serpapi.com/search\"\n",
    "\n",
    "    def search_web(self, query, num_results=10):\n",
    "        params = {\n",
    "            \"api_key\": SERPAPI_KEY,\n",
    "            \"engine\": \"google\",\n",
    "            \"q\": query,\n",
    "            \"num\": num_results,\n",
    "            \"location\": \"United States\",\n",
    "            \"google_domain\": \"google.com\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.search_endpoint, params=params)\n",
    "            results = response.json()\n",
    "            return [r.get(\"link\") for r in results.get(\"organic_results\", [])][:num_results]\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_page_content(self, url):\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                element.decompose()\n",
    "\n",
    "            main_content = soup.find('article') or soup.find('main') or soup.body\n",
    "            paragraphs = main_content.find_all(['p', 'h1', 'h2', 'h3']) if main_content else []\n",
    "            text = '\\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "            return text[:10000]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_answer(self, query, context_chunks):\n",
    "        context = \"\\n\\n\".join([f\"Source {i+1}:\\n{text}\" for i, text in enumerate(context_chunks)])\n",
    "\n",
    "        prompt = f\"\"\"Analyze the following information from web sources and answer the question.\n",
    "        Follow these rules:\n",
    "        1. Be factual and concise\n",
    "        2. Acknowledge conflicting information if present\n",
    "        3. Cite sources using [1], [2] numbering\n",
    "        4. If information is inconsistent, say so\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "    def rag_query(self, query, num_sources=3):\n",
    "        # Web search\n",
    "        urls = self.search_web(query, num_results=num_sources)\n",
    "        if not urls:\n",
    "            return \"No relevant information found through web search\"\n",
    "\n",
    "        # Content retrieval\n",
    "        context_chunks = []\n",
    "        for url in urls:\n",
    "            content = self.fetch_page_content(url)\n",
    "            if content:\n",
    "                context_chunks.append(f\"URL: {url}\\nContent: {content}\")\n",
    "\n",
    "        # Answer generation\n",
    "        return self.generate_answer(query, context_chunks)\n",
    "\n",
    "# Usage\n",
    "rag = SerpApiRAG()\n",
    "answer = rag.rag_query(\"how many parameters are in deepseek r1\")\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
